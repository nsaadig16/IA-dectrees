{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71e1fb64",
   "metadata": {},
   "source": [
    "# Evaluation of models\n",
    "\n",
    "First, get all the functions and the classes from the `dectrees.ipynb` notebook, and copy them to a `tree.py` file. We will use this to import our implementation for this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37fc013d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from tree import DecisionTreeModel, read_csv, split_observations_and_labels, gini, entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dc31f6",
   "metadata": {},
   "source": [
    "Now, we can train our model the same as we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45b52388",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: google?\n",
      "T->3: 18?\n",
      " T->{' Basic\\n': 1}\n",
      " F->3:  18?\n",
      "  T->{' None\\n': 1}\n",
      "  F->{' Premium\\n': 3}\n",
      "F->0: slashdot?\n",
      " T->{' None\\n': 3}\n",
      " F->2:  yes?\n",
      "  T->3: 19?\n",
      "   T->{' Basic': 1}\n",
      "   F->{' Basic\\n': 3}\n",
      "  F->3:  21?\n",
      "   T->{' Basic\\n': 1}\n",
      "   F->{' None\\n': 3}\n"
     ]
    }
   ],
   "source": [
    "dataset = \"decision_tree_example.csv\"\n",
    "csv = read_csv(dataset)\n",
    "data, labels = split_observations_and_labels(csv)\n",
    "\n",
    "model = DecisionTreeModel(gini)\n",
    "model.fit(data, labels)\n",
    "\n",
    "model.tree_.print_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292200bb",
   "metadata": {},
   "source": [
    "Let's try to find the best parameters for our dataset. For this, first we will follow the procedure explained in the course slides:\n",
    "\n",
    "1. Retrieve a wide set of examples\n",
    "2. Divide this set into two sets: the training set and the test set\n",
    "3. Build the classifier with the training set\n",
    "4. Measure the percentage of examples of the test set that are correctly classified\n",
    "5. Repeat steps 2 to 4 for different sizes of training and test sets chosen randomly\n",
    "\n",
    "One iteration of the process might look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c28255f2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['google', ' France', ' yes', '23'], ['digg', ' USA', ' yes', '24'], ['(direct)', ' NewZealand', ' no', ' 12'], ['(direct)', ' UK', ' no', ' 21'], ['google', ' USA', ' no', ' 24'], ['slashdot', ' France', ' yes', '19'], ['digg', ' USA', ' no', ' 18'], ['google', ' UK', ' no', ' 18'], ['digg', ' NewZealand', ' yes', '12'], ['google', ' UK', ' yes', '18']]\n",
      "[['slashdot', ' USA', ' yes', '18'], ['kiwitobes', ' France', ' yes', '23'], ['google', ' UK', ' no', ' 21'], ['kiwitobes', ' UK', ' no', ' 19'], ['slashdot', ' UK', ' no', ' 21'], ['kiwitobes', ' France', ' yes', '19']]\n",
      "[' Premium\\n', ' Basic\\n', ' None\\n', ' Basic\\n', ' Premium\\n', ' None\\n', ' None\\n', ' None\\n', ' Basic\\n', ' Basic\\n']\n",
      "[' None\\n', ' Basic\\n', ' Premium\\n', ' None\\n', ' None\\n', ' Basic']\n"
     ]
    }
   ],
   "source": [
    "def train_test_split(data, labels, train_pctg, rng):\n",
    "    \"\"\"\n",
    "    Splits the input data and their corresponding labels in two\n",
    "    sets, one for training and one for testing, according to a\n",
    "    percentage (`train_pctg`) that specifies how many rows will\n",
    "    end in the training set.\n",
    "\n",
    "    This method will split the rows randomly, so a random generator\n",
    "    is expected (see https://docs.python.org/3/library/random.html#random.Random).\n",
    "    Note that with this class allows you to call the other functions in `random`\n",
    "    as if they were methods from this class (e.g. `rng.randint(10, 20)`).\n",
    "\n",
    "    This method returns the tuple `(train_data, test_data, train_labels, test_labels)`\n",
    "    \"\"\"\n",
    "    idxs = list(range(len(data)))\n",
    "    rng.shuffle(idxs)\n",
    "    \n",
    "    n_train_rows = int(train_pctg * len(data))\n",
    "    train_idxs = idxs[:n_train_rows]\n",
    "\n",
    "    train_data, test_data, train_labels, test_labels = [], [], [], []\n",
    "\n",
    "    for idx, (row, label) in enumerate(zip(data, labels)):\n",
    "        if idx in train_idxs:\n",
    "            train_data.append(row)\n",
    "            train_labels.append(label)\n",
    "        else:\n",
    "            test_data.append(row)\n",
    "            test_labels.append(label)\n",
    "    return train_data, test_data, train_labels, test_labels\n",
    "\n",
    "\n",
    "from random import Random\n",
    "rng = Random(42) # provide a seed to have reproducible results\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, .67, rng)\n",
    "\n",
    "print(train_data)\n",
    "print(test_data)\n",
    "\n",
    "print(train_labels)\n",
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "915270b9-54d2-41b1-881c-209b51b8c2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeModel(gini)\n",
    "model.fit(train_data, train_labels)\n",
    "print(model.score(test_data, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b7cdf1",
   "metadata": {},
   "source": [
    "Implement the full procedure in the `evaluate_model` function. Note that we pass a `model`. The nice thing about our `DecisionTreeModel` class is that it can be \"retrained\" on new data. This \"retrain\" will forget about previous examples, so in essence we are training a model from scratch.\n",
    "\n",
    "This means we do not need to pass the class and the parameters to the `evaluate_model` function, simplifying its definition. **Remember though that the model at the end is trained with the final iteration's data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00186c12",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19999999999999998\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data, labels, iterations, rng) -> float:\n",
    "    score_sum = 0.0\n",
    "    for i in range(iterations):\n",
    "        train_X, test_X, train_y, test_y = train_test_split(data, labels, .67, rng)\n",
    "        model.fit(train_X,train_y)\n",
    "        score_sum += model.score(test_X,test_y)\n",
    "    return score_sum / iterations\n",
    "\n",
    "model = DecisionTreeModel(gini)\n",
    "print(evaluate_model(model, data, labels, 5, Random(42)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657cb979",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Exercise:** Try to use different training sizes for evaluating the model. Modify the `evaluate_model` function to have this as a parameter (with a default value)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a83da9",
   "metadata": {},
   "source": [
    "# Hyper-parameter tuning\n",
    "\n",
    "As we have seen, we have some parameters that modify the algorithm to train the model (e.g. the score function used in the decision tree build process). Those parameters are called **hyper-parameters**. By modifying those parameters we can impact the result of the algorithm, changing the generalization capabilities of the trained models.\n",
    "\n",
    "We define as **hyper-parameter tuning** the process where we try to automatically select which values we set to those *hyper-parameters* such that we maximize the prediction capabilities on unseen data.\n",
    "\n",
    "For this, we first need to define the **parameter space**, which is the domain (the valid values) of each hyper-parameter.\n",
    "\n",
    "Let's define the parameter space for our `DecisionTreeModel` class:\n",
    "\n",
    "- `scorefn`: $\\{gini, entropy\\}$\n",
    "- `beta`: $[0, \\infty)$ (note that different `scorefn` might result in impurities larger than 1, so we don't have an upper bound).\n",
    "- `prune_threshold`: $[0, \\infty)$\n",
    "\n",
    "For now, we will define a discrete set of values for each parameter, and consider any possible combination of those values (i.e. the cartesian product)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9fff04a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "parameter_space = {\n",
    "    \"scoref\": [gini, entropy],\n",
    "    \"beta\": [0, 0.1], # try some values\n",
    "    \"prune_threshold\": [0, 0.1], # try some values\n",
    "}\n",
    "\n",
    "import itertools\n",
    "def iterate_parametrizations(pspace):\n",
    "    names = list(pspace.keys())\n",
    "    for values in itertools.product(*pspace.values()):\n",
    "        yield dict(zip(names, values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a8c4bc",
   "metadata": {},
   "source": [
    "We can see all the possible parametrizations with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcc6ebad",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'scoref': <function gini at 0x79bbe0352200>, 'beta': 0, 'prune_threshold': 0}\n",
      "{'scoref': <function gini at 0x79bbe0352200>, 'beta': 0, 'prune_threshold': 0.1}\n",
      "{'scoref': <function gini at 0x79bbe0352200>, 'beta': 0.1, 'prune_threshold': 0}\n",
      "{'scoref': <function gini at 0x79bbe0352200>, 'beta': 0.1, 'prune_threshold': 0.1}\n",
      "{'scoref': <function entropy at 0x79bbe03522a0>, 'beta': 0, 'prune_threshold': 0}\n",
      "{'scoref': <function entropy at 0x79bbe03522a0>, 'beta': 0, 'prune_threshold': 0.1}\n",
      "{'scoref': <function entropy at 0x79bbe03522a0>, 'beta': 0.1, 'prune_threshold': 0}\n",
      "{'scoref': <function entropy at 0x79bbe03522a0>, 'beta': 0.1, 'prune_threshold': 0.1}\n"
     ]
    }
   ],
   "source": [
    "for params in iterate_parametrizations(parameter_space):\n",
    "    print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07299a69",
   "metadata": {},
   "source": [
    "Now let's define our evaluate function, and try to find the best parameters for our example dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c8fe244",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "evaluate_model() missing 3 required positional arguments: 'labels', 'iterations', and 'rng'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      7\u001b[39m         ...\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m best_params, best_score\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m best, score = \u001b[43mselect_best_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameter_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRandom\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest parameters for our model are\u001b[39m\u001b[33m\"\u001b[39m, best, \u001b[33m\"\u001b[39m\u001b[33mwith an score of\u001b[39m\u001b[33m\"\u001b[39m, score)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mselect_best_parameters\u001b[39m\u001b[34m(data, labels, parameter_space, rng)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m params \u001b[38;5;129;01min\u001b[39;00m iterate_parametrizations(parameter_space):\n\u001b[32m      5\u001b[39m     model = DecisionTreeModel(**params)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     score = \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     ...\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m best_params, best_score\n",
      "\u001b[31mTypeError\u001b[39m: evaluate_model() missing 3 required positional arguments: 'labels', 'iterations', and 'rng'"
     ]
    }
   ],
   "source": [
    "def select_best_parameters(data, labels, parameter_space, rng):\n",
    "    best_params, best_score = None, 0.0\n",
    "\n",
    "    for params in iterate_parametrizations(parameter_space):\n",
    "        model = DecisionTreeModel(**params)\n",
    "        score = evaluate_model(model, ...)\n",
    "        ...\n",
    "    \n",
    "    return best_params, best_score\n",
    "\n",
    "\n",
    "best, score = select_best_parameters(data, labels, parameter_space, Random(42))\n",
    "print(\"Best parameters for our model are\", best, \"with an score of\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b7addc",
   "metadata": {},
   "source": [
    "# Cross validation\n",
    "Another way to select the best parameters is to perform the cross validation procedure.\n",
    "\n",
    "![CV procedure](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png).\n",
    "\n",
    "The process is similar to what we have seen, but instead of doing N iterations over different random splits, we separate the training data in different splits. Then, at each iteration, we use one split as the test data, and combine the other ones as the train data. We do this until we have used all the splits as the test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e73b94",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def cross_val_score(model, data, labels, rng, k):\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abe3722",
   "metadata": {},
   "source": [
    "**Exercise:** Modify the `select_best_parameters` function to accept a scoring function. Note that different scoring functions can have different parameters, but they must accept at least `model, data, labels`.\n",
    "\n",
    "Option 1: Use lambda functions (or [partial](https://docs.python.org/3/library/functools.html#functools.partial) for a cleaner approach) to define a function with the other parameters set, and then have `select_best_parameters` to call only `fn(model, data, labels)`:\n",
    "\n",
    "```python\n",
    "def select_best_parameters(data, labels, pspace, fn):\n",
    "    ...\n",
    "    fn(model, data, labels)\n",
    "    ...\n",
    "\n",
    "... = select_best_parameters(data, labels, pspace, lambda m,d,l: evaluate_model(m, d, l, 5, Random(42)))\n",
    "```\n",
    "\n",
    "Option 2: Use `*args, **kwargs` in `select_best_parameters` to pass additional parameters:\n",
    "```python\n",
    "def select_best_parameters(data, labels, pspace, fn, *args, **kwargs):\n",
    "    ...\n",
    "    fn(model, data, labels, *args, **kwargs)\n",
    "    ...\n",
    "\n",
    "... = select_best_parameters(data, labels, pspace, evaluate_model, 5, Random(42))\n",
    "```\n",
    "\n",
    "\n",
    "Try now to select the best parameters using cross validation instead."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
